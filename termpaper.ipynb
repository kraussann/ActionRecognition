{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "курсач господи прости.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DleP2qL_Io0q"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "#from scipy.misc.pilutil import imread\n",
        "#from scipy.misc.pilutil import imshow\n",
        "import glob\n",
        "import cv2\n",
        "import copy\n",
        "import math\n",
        "from google.colab.patches import cv2_imshow\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from math import sqrt\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import pairwise_distances_argmin_min\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score as cv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYVKW4xDGECE"
      },
      "source": [
        "#1st frame from a video\n",
        "def getFirstFrame(videofile):\n",
        "    vidcap = cv2.VideoCapture(videofile)\n",
        "    success, image = vidcap.read()\n",
        "    if success:\n",
        "        cv2.imwrite(\"first_frame.jpg\", image)\n",
        "\n",
        "getFirstFrame('/content/hand001-detect.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i30qPoHREzUy"
      },
      "source": [
        "\n",
        "def click_event(event, x, y, flags, params):\n",
        "  \n",
        "    if event == cv2.EVENT_LBUTTONDOWN:\n",
        "  \n",
        "        print(x, ' ', y)\n",
        "\n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        cv2.putText(img, str(x) + ',' +\n",
        "                    str(y), (x,y), font,\n",
        "                    1, (255, 0, 0), 2)\n",
        "        cv2_imshow(img)    \n",
        "    if event==cv2.EVENT_RBUTTONDOWN:\n",
        "  \n",
        "\n",
        "        print(x, ' ', y)\n",
        "  \n",
        "   \n",
        "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "        b = img[y, x, 0]\n",
        "        g = img[y, x, 1]\n",
        "        r = img[y, x, 2]\n",
        "        cv2.putText(img, str(b) + ',' +\n",
        "                    str(g) + ',' + str(r),\n",
        "                    (x,y), font, 1,\n",
        "                    (255, 255, 0), 2)\n",
        "        cv2_imshow(img)\n",
        "  \n",
        "\n",
        "if __name__==\"__main__\":\n",
        "  \n",
        "    img = cv2.imread('first_frame.jpg', 1)\n",
        "  \n",
        "    cv2_imshow(img)\n",
        "  \n",
        "\n",
        "    cv2.setMouseCallback('image', click_event)\n",
        "  \n",
        "\n",
        "    cv2.waitKey(0)\n",
        "  \n",
        "    cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a851VEFF6bA1"
      },
      "source": [
        "##3D by Monte Carlo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxkrO_LrUphM"
      },
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "class Point2:\n",
        "    def __init__(self, x=0, y=0):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "class Point3:\n",
        "    def __init__(self, x=0, y=0, z=0):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.z = z\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIEYJiPcTWlL"
      },
      "source": [
        "i0 = Point2(278, 244)\n",
        "i1 = Point2(460, 103)\n",
        "i2 = Point2(574, 105)\n",
        "i3 = Point2(447, 276)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uam7hhtzR_l2"
      },
      "source": [
        "r0 = Point3(0, 0, 0)\n",
        "r1 = Point3(0, 0, 1)\n",
        "r2 = Point3(1, 0, 0)\n",
        "r3 = Point3(1, 0, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMnxngWWUPah"
      },
      "source": [
        "mat = [\n",
        "    [1, 0, 0, 0],\n",
        "    [0, 1, 0, 0],\n",
        "    [0, 0, 1, 0],\n",
        "    [0, 0, 0, 1],\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BoZS_y0g3u3",
        "outputId": "b4fddd75-09af-474f-8928-3d25726f0b06"
      },
      "source": [
        "type(r1.z)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UGkSYqAd9sQ",
        "outputId": "19566285-723c-406a-9a73-6c3e2ce61e91"
      },
      "source": [
        "mat[1][3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "int"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja9DGC-6UWGN"
      },
      "source": [
        "def project(p, mat):\n",
        "    x = mat[0][0] * p.x + mat[0][1] * p.y + mat[0][2] * p.z + mat[0][3] * 1\n",
        "    y = mat[1][0] * p.x + mat[1][1] * p.y + mat[1][2] * p.z + mat[1][3] * 1\n",
        "    w = mat[3][0] * p.x + mat[3][1] * p.y + mat[3][2] * p.z + mat[3][3] * 1\n",
        "    return Point2(720 * (x / w + 1) / 2., 576 - 576 * (y / w + 1) / 2.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVmJWvYfWSOy"
      },
      "source": [
        "def norm2(a, b):\n",
        "    dx = b.x - a.x\n",
        "    dy = b.y - a.y\n",
        "    return dx * dx + dy * dy\n",
        "\n",
        "def evaluate(mat): \n",
        "    c0 = project(r0, mat)\n",
        "    c1 = project(r1, mat)\n",
        "    c2 = project(r2, mat)\n",
        "    c3 = project(r3, mat)\n",
        "    return norm2(i0, c0) + norm2(i1, c1) + norm2(i2, c2) + norm2(i3, c3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwRX8Tw9WUKO"
      },
      "source": [
        "def perturb(mat, amount):\n",
        "    from copy import deepcopy\n",
        "    from random import randrange, uniform\n",
        "    mat2 = deepcopy(mat)\n",
        "    mat2[randrange(4)][randrange(4)] += uniform(-amount, amount)\n",
        "    return mat2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJwB6Ya_WZka"
      },
      "source": [
        "def approximate(mat, amount, n=100000):\n",
        "    est = evaluate(mat)\n",
        "\n",
        "    for i in range(n):\n",
        "        mat2 = perturb(mat, amount)\n",
        "        est2 = evaluate(mat2)\n",
        "        if est2 < est:\n",
        "            mat = mat2\n",
        "            est = est2\n",
        "\n",
        "    return mat, est"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPvOsnT8kY8h",
        "outputId": "d41e0e5e-50a1-420f-8c58-7182e9ffbb08"
      },
      "source": [
        "mat[0][1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.31085854234935106, 1, 153.17115957670615, 0.3293731169230514]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlzYvRBoWcgS"
      },
      "source": [
        "for i in range(100):\n",
        "    mat = approximate(mat, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crUAm7fool_9"
      },
      "source": [
        "import glLoadMatrixf\n",
        "def transpose(m):\n",
        "    return [\n",
        "        [m[0][0], m[1][0], m[2][0], m[3][0]],\n",
        "        [m[0][1], m[1][1], m[2][1], m[3][1]],\n",
        "        [m[0][2], m[1][2], m[2][2], m[3][2]],\n",
        "        [m[0][3], m[1][3], m[2][3], m[3][3]],\n",
        "    ]\n",
        "\n",
        "glLoadMatrixf(transpose(mat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DjH_xUVJ1On"
      },
      "source": [
        "cap = cv2.VideoCapture('hand001-detect.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AvvsgTkOSMj"
      },
      "source": [
        "import cv2\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument('-i', '--image', required=True,\n",
        "                help = 'path to input image')\n",
        "ap.add_argument('-c', '--config', required=True,\n",
        "                help = 'path to yolo config file')\n",
        "ap.add_argument('-w', '--weights', required=True,\n",
        "                help = 'path to yolo pre-trained weights')\n",
        "ap.add_argument('-cl', '--classes', required=True,\n",
        "                help = 'path to text file containing class names')\n",
        "args = ap.parse_args()\n",
        "\n",
        "\n",
        "def get_output_layers(net):\n",
        "    \n",
        "    layer_names = net.getLayerNames()\n",
        "    \n",
        "    output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "    return output_layers\n",
        "\n",
        "\n",
        "def draw_prediction(img, class_id, confidence, x, y, x_plus_w, y_plus_h):\n",
        "\n",
        "    label = str(classes[class_id])\n",
        "\n",
        "    color = COLORS[class_id]\n",
        "\n",
        "    cv2.rectangle(img, (x,y), (x_plus_w,y_plus_h), color, 2)\n",
        "\n",
        "    cv2.putText(img, label, (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "    \n",
        "image = cv2.imread(args.image)\n",
        "\n",
        "Width = image.shape[1]\n",
        "Height = image.shape[0]\n",
        "scale = 0.00392\n",
        "\n",
        "classes = None\n",
        "\n",
        "with open(args.classes, 'r') as f:\n",
        "    classes = [line.strip() for line in f.readlines()]\n",
        "\n",
        "COLORS = np.random.uniform(0, 255, size=(len(classes), 3))\n",
        "\n",
        "net = cv2.dnn.readNet(args.weights, args.config)\n",
        "\n",
        "blob = cv2.dnn.blobFromImage(image, scale, (416,416), (0,0,0), True, crop=False)\n",
        "\n",
        "net.setInput(blob)\n",
        "\n",
        "outs = net.forward(get_output_layers(net))\n",
        "\n",
        "class_ids = []\n",
        "confidences = []\n",
        "boxes = []\n",
        "conf_threshold = 0.5\n",
        "nms_threshold = 0.4\n",
        "\n",
        "\n",
        "for out in outs:\n",
        "    for detection in out:\n",
        "        scores = detection[5:]\n",
        "        class_id = np.argmax(scores)\n",
        "        confidence = scores[class_id]\n",
        "        if confidence > 0.5:\n",
        "            center_x = int(detection[0] * Width)\n",
        "            center_y = int(detection[1] * Height)\n",
        "            w = int(detection[2] * Width)\n",
        "            h = int(detection[3] * Height)\n",
        "            x = center_x - w / 2\n",
        "            y = center_y - h / 2\n",
        "            class_ids.append(class_id)\n",
        "            confidences.append(float(confidence))\n",
        "            boxes.append([x, y, w, h])\n",
        "\n",
        "\n",
        "indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)\n",
        "\n",
        "for i in indices:\n",
        "    i = i[0]\n",
        "    box = boxes[i]\n",
        "    x = box[0]\n",
        "    y = box[1]\n",
        "    w = box[2]\n",
        "    h = box[3]\n",
        "    draw_prediction(image, class_ids[i], confidences[i], round(x), round(y), round(x+w), round(y+h))\n",
        "\n",
        "cv2.imshow(\"object detection\", image)\n",
        "cv2.waitKey()\n",
        "    \n",
        "cv2.imwrite(\"object-detection.jpg\", image)\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "vkUxhuTsblFo",
        "outputId": "7869b682-07b9-47f3-f522-e26abbab2cf8"
      },
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "img = cv.imread('1stframe.png',0)\n",
        "ret, thresh = cv.threshold(img,127,255,0)\n",
        "im2,contours,hierarchy = cv.findContours(thresh, 1, 2)\n",
        "cnt = contours[0]\n",
        "M = cv.moments(cnt)\n",
        "print( M )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c6babeb4704d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1stframe.png'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m127\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mim2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcontours\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhierarchy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindContours\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mcnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontours\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqqQhC7U6Oku"
      },
      "source": [
        "##3D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNMm4xA4cBrM"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch import autograd\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.parallel import DistributedDataParallel\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from mvn.models.triangulation import RANSACTriangulationNet, AlgebraicTriangulationNet, VolumetricTriangulationNet\n",
        "from mvn.models.loss import KeypointsMSELoss, KeypointsMSESmoothLoss, KeypointsMAELoss, KeypointsL2Loss, VolumetricCELoss\n",
        "\n",
        "from mvn.utils import img, multiview, op, vis, misc, cfg\n",
        "from mvn.datasets import human36m\n",
        "from mvn.datasets import utils as dataset_utils\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"--config\", type=str, required=True, help=\"Path, where config file is stored\")\n",
        "    parser.add_argument('--eval', action='store_true', help=\"If set, then only evaluation will be done\")\n",
        "    parser.add_argument('--eval_dataset', type=str, default='val', help=\"Dataset split on which evaluate. Can be 'train' and 'val'\")\n",
        "\n",
        "    parser.add_argument(\"--local_rank\", type=int, help=\"Local rank of the process on the node\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed for reproducibility\")\n",
        "\n",
        "    parser.add_argument(\"--logdir\", type=str, default=\"/Vol1/dbstore/datasets/k.iskakov/logs/multi-view-net-repr\", help=\"Path, where logs will be stored\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def setup_human36m_dataloaders(config, is_train, distributed_train):\n",
        "    train_dataloader = None\n",
        "    if is_train:\n",
        "        # train\n",
        "        train_dataset = human36m.Human36MMultiViewDataset(\n",
        "            h36m_root=config.dataset.train.h36m_root,\n",
        "            pred_results_path=config.dataset.train.pred_results_path if hasattr(config.dataset.train, \"pred_results_path\") else None,\n",
        "            train=True,\n",
        "            test=False,\n",
        "            image_shape=config.image_shape if hasattr(config, \"image_shape\") else (256, 256),\n",
        "            labels_path=config.dataset.train.labels_path,\n",
        "            with_damaged_actions=config.dataset.train.with_damaged_actions,\n",
        "            scale_bbox=config.dataset.train.scale_bbox,\n",
        "            kind=config.kind,\n",
        "            undistort_images=config.dataset.train.undistort_images,\n",
        "            ignore_cameras=config.dataset.train.ignore_cameras if hasattr(config.dataset.train, \"ignore_cameras\") else [],\n",
        "            crop=config.dataset.train.crop if hasattr(config.dataset.train, \"crop\") else True,\n",
        "        )\n",
        "\n",
        "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset) if distributed_train else None\n",
        "\n",
        "        train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=config.opt.batch_size,\n",
        "            shuffle=config.dataset.train.shuffle and (train_sampler is None), # debatable\n",
        "            sampler=train_sampler,\n",
        "            collate_fn=dataset_utils.make_collate_fn(randomize_n_views=config.dataset.train.randomize_n_views,\n",
        "                                                     min_n_views=config.dataset.train.min_n_views,\n",
        "                                                     max_n_views=config.dataset.train.max_n_views),\n",
        "            num_workers=config.dataset.train.num_workers,\n",
        "            worker_init_fn=dataset_utils.worker_init_fn,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "    # val\n",
        "    val_dataset = human36m.Human36MMultiViewDataset(\n",
        "        h36m_root=config.dataset.val.h36m_root,\n",
        "        pred_results_path=config.dataset.val.pred_results_path if hasattr(config.dataset.val, \"pred_results_path\") else None,\n",
        "        train=False,\n",
        "        test=True,\n",
        "        image_shape=config.image_shape if hasattr(config, \"image_shape\") else (256, 256),\n",
        "        labels_path=config.dataset.val.labels_path,\n",
        "        with_damaged_actions=config.dataset.val.with_damaged_actions,\n",
        "        retain_every_n_frames_in_test=config.dataset.val.retain_every_n_frames_in_test,\n",
        "        scale_bbox=config.dataset.val.scale_bbox,\n",
        "        kind=config.kind,\n",
        "        undistort_images=config.dataset.val.undistort_images,\n",
        "        ignore_cameras=config.dataset.val.ignore_cameras if hasattr(config.dataset.val, \"ignore_cameras\") else [],\n",
        "        crop=config.dataset.val.crop if hasattr(config.dataset.val, \"crop\") else True,\n",
        "    )\n",
        "\n",
        "    val_dataloader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config.opt.val_batch_size if hasattr(config.opt, \"val_batch_size\") else config.opt.batch_size,\n",
        "        shuffle=config.dataset.val.shuffle,\n",
        "        collate_fn=dataset_utils.make_collate_fn(randomize_n_views=config.dataset.val.randomize_n_views,\n",
        "                                                 min_n_views=config.dataset.val.min_n_views,\n",
        "                                                 max_n_views=config.dataset.val.max_n_views),\n",
        "        num_workers=config.dataset.val.num_workers,\n",
        "        worker_init_fn=dataset_utils.worker_init_fn,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_dataloader, val_dataloader, train_sampler\n",
        "\n",
        "\n",
        "def setup_dataloaders(config, is_train=True, distributed_train=False):\n",
        "    if config.dataset.kind == 'human36m':\n",
        "        train_dataloader, val_dataloader, train_sampler = setup_human36m_dataloaders(config, is_train, distributed_train)\n",
        "    else:\n",
        "        raise NotImplementedError(\"Unknown dataset: {}\".format(config.dataset.kind))\n",
        "\n",
        "    return train_dataloader, val_dataloader, train_sampler\n",
        "\n",
        "\n",
        "def setup_experiment(config, model_name, is_train=True):\n",
        "    prefix = \"\" if is_train else \"eval_\"\n",
        "\n",
        "    if config.title:\n",
        "        experiment_title = config.title + \"_\" + model_name\n",
        "    else:\n",
        "        experiment_title = model_name\n",
        "\n",
        "    experiment_title = prefix + experiment_title\n",
        "\n",
        "    experiment_name = '{}@{}'.format(experiment_title, datetime.now().strftime(\"%d.%m.%Y-%H:%M:%S\"))\n",
        "    print(\"Experiment name: {}\".format(experiment_name))\n",
        "\n",
        "    experiment_dir = os.path.join(args.logdir, experiment_name)\n",
        "    os.makedirs(experiment_dir, exist_ok=True)\n",
        "\n",
        "    checkpoints_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
        "    os.makedirs(checkpoints_dir, exist_ok=True)\n",
        "\n",
        "    shutil.copy(args.config, os.path.join(experiment_dir, \"config.yaml\"))\n",
        "\n",
        "    # tensorboard\n",
        "    writer = SummaryWriter(os.path.join(experiment_dir, \"tb\"))\n",
        "\n",
        "    # dump config to tensorboard\n",
        "    writer.add_text(misc.config_to_str(config), \"config\", 0)\n",
        "\n",
        "    return experiment_dir, writer\n",
        "\n",
        "\n",
        "def one_epoch(model, criterion, opt, config, dataloader, device, epoch, n_iters_total=0, is_train=True, caption='', master=False, experiment_dir=None, writer=None):\n",
        "    name = \"train\" if is_train else \"val\"\n",
        "    model_type = config.model.name\n",
        "\n",
        "    if is_train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    metric_dict = defaultdict(list)\n",
        "\n",
        "    results = defaultdict(list)\n",
        "\n",
        "    # used to turn on/off gradients\n",
        "    grad_context = torch.autograd.enable_grad if is_train else torch.no_grad\n",
        "    with grad_context():\n",
        "        end = time.time()\n",
        "\n",
        "        iterator = enumerate(dataloader)\n",
        "        if is_train and config.opt.n_iters_per_epoch is not None:\n",
        "            iterator = islice(iterator, config.opt.n_iters_per_epoch)\n",
        "\n",
        "        for iter_i, batch in iterator:\n",
        "            with autograd.detect_anomaly():\n",
        "                # measure data loading time\n",
        "                data_time = time.time() - end\n",
        "\n",
        "                if batch is None:\n",
        "                    print(\"Found None batch\")\n",
        "                    continue\n",
        "\n",
        "                images_batch, keypoints_3d_gt, keypoints_3d_validity_gt, proj_matricies_batch = dataset_utils.prepare_batch(batch, device, config)\n",
        "\n",
        "                keypoints_2d_pred, cuboids_pred, base_points_pred = None, None, None\n",
        "                if model_type == \"alg\" or model_type == \"ransac\":\n",
        "                    keypoints_3d_pred, keypoints_2d_pred, heatmaps_pred, confidences_pred = model(images_batch, proj_matricies_batch, batch)\n",
        "                elif model_type == \"vol\":\n",
        "                    keypoints_3d_pred, heatmaps_pred, volumes_pred, confidences_pred, cuboids_pred, coord_volumes_pred, base_points_pred = model(images_batch, proj_matricies_batch, batch)\n",
        "\n",
        "                batch_size, n_views, image_shape = images_batch.shape[0], images_batch.shape[1], tuple(images_batch.shape[3:])\n",
        "                n_joints = keypoints_3d_pred.shape[1]\n",
        "\n",
        "                keypoints_3d_binary_validity_gt = (keypoints_3d_validity_gt > 0.0).type(torch.float32)\n",
        "\n",
        "                scale_keypoints_3d = config.opt.scale_keypoints_3d if hasattr(config.opt, \"scale_keypoints_3d\") else 1.0\n",
        "\n",
        "                # 1-view case\n",
        "                if n_views == 1:\n",
        "                    if config.kind == \"human36m\":\n",
        "                        base_joint = 6\n",
        "                    elif config.kind == \"coco\":\n",
        "                        base_joint = 11\n",
        "\n",
        "                    keypoints_3d_gt_transformed = keypoints_3d_gt.clone()\n",
        "                    keypoints_3d_gt_transformed[:, torch.arange(n_joints) != base_joint] -= keypoints_3d_gt_transformed[:, base_joint:base_joint + 1]\n",
        "                    keypoints_3d_gt = keypoints_3d_gt_transformed\n",
        "\n",
        "                    keypoints_3d_pred_transformed = keypoints_3d_pred.clone()\n",
        "                    keypoints_3d_pred_transformed[:, torch.arange(n_joints) != base_joint] -= keypoints_3d_pred_transformed[:, base_joint:base_joint + 1]\n",
        "                    keypoints_3d_pred = keypoints_3d_pred_transformed\n",
        "\n",
        "                # calculate loss\n",
        "                total_loss = 0.0\n",
        "                loss = criterion(keypoints_3d_pred * scale_keypoints_3d, keypoints_3d_gt * scale_keypoints_3d, keypoints_3d_binary_validity_gt)\n",
        "                total_loss += loss\n",
        "                metric_dict[f'{config.opt.criterion}'].append(loss.item())\n",
        "\n",
        "                # volumetric ce loss\n",
        "                use_volumetric_ce_loss = config.opt.use_volumetric_ce_loss if hasattr(config.opt, \"use_volumetric_ce_loss\") else False\n",
        "                if use_volumetric_ce_loss:\n",
        "                    volumetric_ce_criterion = VolumetricCELoss()\n",
        "\n",
        "                    loss = volumetric_ce_criterion(coord_volumes_pred, volumes_pred, keypoints_3d_gt, keypoints_3d_binary_validity_gt)\n",
        "                    metric_dict['volumetric_ce_loss'].append(loss.item())\n",
        "\n",
        "                    weight = config.opt.volumetric_ce_loss_weight if hasattr(config.opt, \"volumetric_ce_loss_weight\") else 1.0\n",
        "                    total_loss += weight * loss\n",
        "\n",
        "                metric_dict['total_loss'].append(total_loss.item())\n",
        "\n",
        "                if is_train:\n",
        "                    opt.zero_grad()\n",
        "                    total_loss.backward()\n",
        "\n",
        "                    if hasattr(config.opt, \"grad_clip\"):\n",
        "                        torch.nn.utils.clip_grad_norm_(model.parameters(), config.opt.grad_clip / config.opt.lr)\n",
        "\n",
        "                    metric_dict['grad_norm_times_lr'].append(config.opt.lr * misc.calc_gradient_norm(filter(lambda x: x[1].requires_grad, model.named_parameters())))\n",
        "\n",
        "                    opt.step()\n",
        "\n",
        "                # calculate metrics\n",
        "                l2 = KeypointsL2Loss()(keypoints_3d_pred * scale_keypoints_3d, keypoints_3d_gt * scale_keypoints_3d, keypoints_3d_binary_validity_gt)\n",
        "                metric_dict['l2'].append(l2.item())\n",
        "\n",
        "                # base point l2\n",
        "                if base_points_pred is not None:\n",
        "                    base_point_l2_list = []\n",
        "                    for batch_i in range(batch_size):\n",
        "                        base_point_pred = base_points_pred[batch_i]\n",
        "\n",
        "                        if config.model.kind == \"coco\":\n",
        "                            base_point_gt = (keypoints_3d_gt[batch_i, 11, :3] + keypoints_3d[batch_i, 12, :3]) / 2\n",
        "                        elif config.model.kind == \"mpii\":\n",
        "                            base_point_gt = keypoints_3d_gt[batch_i, 6, :3]\n",
        "\n",
        "                        base_point_l2_list.append(torch.sqrt(torch.sum((base_point_pred * scale_keypoints_3d - base_point_gt * scale_keypoints_3d) ** 2)).item())\n",
        "\n",
        "                    base_point_l2 = 0.0 if len(base_point_l2_list) == 0 else np.mean(base_point_l2_list)\n",
        "                    metric_dict['base_point_l2'].append(base_point_l2)\n",
        "\n",
        "                # save answers for evalulation\n",
        "                if not is_train:\n",
        "                    results['keypoints_3d'].append(keypoints_3d_pred.detach().cpu().numpy())\n",
        "                    results['indexes'].append(batch['indexes'])\n",
        "\n",
        "                # plot visualization\n",
        "                if master:\n",
        "                    if n_iters_total % config.vis_freq == 0:# or total_l2.item() > 500.0:\n",
        "                        vis_kind = config.kind\n",
        "                        if (config.transfer_cmu_to_human36m if hasattr(config, \"transfer_cmu_to_human36m\") else False):\n",
        "                            vis_kind = \"coco\"\n",
        "\n",
        "                        for batch_i in range(min(batch_size, config.vis_n_elements)):\n",
        "                            keypoints_vis = vis.visualize_batch(\n",
        "                                images_batch, heatmaps_pred, keypoints_2d_pred, proj_matricies_batch,\n",
        "                                keypoints_3d_gt, keypoints_3d_pred,\n",
        "                                kind=vis_kind,\n",
        "                                cuboids_batch=cuboids_pred,\n",
        "                                confidences_batch=confidences_pred,\n",
        "                                batch_index=batch_i, size=5,\n",
        "                                max_n_cols=10\n",
        "                            )\n",
        "                            writer.add_image(f\"{name}/keypoints_vis/{batch_i}\", keypoints_vis.transpose(2, 0, 1), global_step=n_iters_total)\n",
        "\n",
        "                            heatmaps_vis = vis.visualize_heatmaps(\n",
        "                                images_batch, heatmaps_pred,\n",
        "                                kind=vis_kind,\n",
        "                                batch_index=batch_i, size=5,\n",
        "                                max_n_rows=10, max_n_cols=10\n",
        "                            )\n",
        "                            writer.add_image(f\"{name}/heatmaps/{batch_i}\", heatmaps_vis.transpose(2, 0, 1), global_step=n_iters_total)\n",
        "\n",
        "                            if model_type == \"vol\":\n",
        "                                volumes_vis = vis.visualize_volumes(\n",
        "                                    images_batch, volumes_pred, proj_matricies_batch,\n",
        "                                    kind=vis_kind,\n",
        "                                    cuboids_batch=cuboids_pred,\n",
        "                                    batch_index=batch_i, size=5,\n",
        "                                    max_n_rows=1, max_n_cols=16\n",
        "                                )\n",
        "                                writer.add_image(f\"{name}/volumes/{batch_i}\", volumes_vis.transpose(2, 0, 1), global_step=n_iters_total)\n",
        "\n",
        "                    # dump weights to tensoboard\n",
        "                    if n_iters_total % config.vis_freq == 0:\n",
        "                        for p_name, p in model.named_parameters():\n",
        "                            try:\n",
        "                                writer.add_histogram(p_name, p.clone().cpu().data.numpy(), n_iters_total)\n",
        "                            except ValueError as e:\n",
        "                                print(e)\n",
        "                                print(p_name, p)\n",
        "                                exit()\n",
        "\n",
        "                    # dump to tensorboard per-iter loss/metric stats\n",
        "                    if is_train:\n",
        "                        for title, value in metric_dict.items():\n",
        "                            writer.add_scalar(f\"{name}/{title}\", value[-1], n_iters_total)\n",
        "\n",
        "                    # measure elapsed time\n",
        "                    batch_time = time.time() - end\n",
        "                    end = time.time()\n",
        "\n",
        "                    # dump to tensorboard per-iter time stats\n",
        "                    writer.add_scalar(f\"{name}/batch_time\", batch_time, n_iters_total)\n",
        "                    writer.add_scalar(f\"{name}/data_time\", data_time, n_iters_total)\n",
        "\n",
        "                    # dump to tensorboard per-iter stats about sizes\n",
        "                    writer.add_scalar(f\"{name}/batch_size\", batch_size, n_iters_total)\n",
        "                    writer.add_scalar(f\"{name}/n_views\", n_views, n_iters_total)\n",
        "\n",
        "                    n_iters_total += 1\n",
        "\n",
        "    # calculate evaluation metrics\n",
        "    if master:\n",
        "        if not is_train:\n",
        "            results['keypoints_3d'] = np.concatenate(results['keypoints_3d'], axis=0)\n",
        "            results['indexes'] = np.concatenate(results['indexes'])\n",
        "\n",
        "            try:\n",
        "                scalar_metric, full_metric = dataloader.dataset.evaluate(results['keypoints_3d'])\n",
        "            except Exception as e:\n",
        "                print(\"Failed to evaluate. Reason: \", e)\n",
        "                scalar_metric, full_metric = 0.0, {}\n",
        "\n",
        "            metric_dict['dataset_metric'].append(scalar_metric)\n",
        "\n",
        "            checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\", \"{:04}\".format(epoch))\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "            # dump results\n",
        "            with open(os.path.join(checkpoint_dir, \"results.pkl\"), 'wb') as fout:\n",
        "                pickle.dump(results, fout)\n",
        "\n",
        "            # dump full metric\n",
        "            with open(os.path.join(checkpoint_dir, \"metric.json\".format(epoch)), 'w') as fout:\n",
        "                json.dump(full_metric, fout, indent=4, sort_keys=True)\n",
        "\n",
        "        # dump to tensorboard per-epoch stats\n",
        "        for title, value in metric_dict.items():\n",
        "            writer.add_scalar(f\"{name}/{title}_epoch\", np.mean(value), epoch)\n",
        "\n",
        "    return n_iters_total\n",
        "\n",
        "\n",
        "def init_distributed(args):\n",
        "    if \"WORLD_SIZE\" not in os.environ or int(os.environ[\"WORLD_SIZE\"]) < 1:\n",
        "        return False\n",
        "\n",
        "    torch.cuda.set_device(args.local_rank)\n",
        "\n",
        "    assert os.environ[\"MASTER_PORT\"], \"set the MASTER_PORT variable or use pytorch launcher\"\n",
        "    assert os.environ[\"RANK\"], \"use pytorch launcher and explicityly state the rank of the process\"\n",
        "\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.distributed.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    print(\"Number of available GPUs: {}\".format(torch.cuda.device_count()))\n",
        "\n",
        "    is_distributed = init_distributed(args)\n",
        "    master = True\n",
        "    if is_distributed and os.environ[\"RANK\"]:\n",
        "        master = int(os.environ[\"RANK\"]) == 0\n",
        "\n",
        "    if is_distributed:\n",
        "        device = torch.device(args.local_rank)\n",
        "    else:\n",
        "        device = torch.device(0)\n",
        "\n",
        "    # config\n",
        "    config = cfg.load_config(args.config)\n",
        "    config.opt.n_iters_per_epoch = config.opt.n_objects_per_epoch // config.opt.batch_size\n",
        "\n",
        "    model = {\n",
        "        \"ransac\": RANSACTriangulationNet,\n",
        "        \"alg\": AlgebraicTriangulationNet,\n",
        "        \"vol\": VolumetricTriangulationNet\n",
        "    }[config.model.name](config, device=device).to(device)\n",
        "\n",
        "    if config.model.init_weights:\n",
        "        state_dict = torch.load(config.model.checkpoint)\n",
        "        for key in list(state_dict.keys()):\n",
        "            new_key = key.replace(\"module.\", \"\")\n",
        "            state_dict[new_key] = state_dict.pop(key)\n",
        "\n",
        "        model.load_state_dict(state_dict, strict=True)\n",
        "        print(\"Successfully loaded pretrained weights for whole model\")\n",
        "\n",
        "    # criterion\n",
        "    criterion_class = {\n",
        "        \"MSE\": KeypointsMSELoss,\n",
        "        \"MSESmooth\": KeypointsMSESmoothLoss,\n",
        "        \"MAE\": KeypointsMAELoss\n",
        "    }[config.opt.criterion]\n",
        "\n",
        "    if config.opt.criterion == \"MSESmooth\":\n",
        "        criterion = criterion_class(config.opt.mse_smooth_threshold)\n",
        "    else:\n",
        "        criterion = criterion_class()\n",
        "\n",
        "    # optimizer\n",
        "    opt = None\n",
        "    if not args.eval:\n",
        "        if config.model.name == \"vol\":\n",
        "            opt = torch.optim.Adam(\n",
        "                [{'params': model.backbone.parameters()},\n",
        "                 {'params': model.process_features.parameters(), 'lr': config.opt.process_features_lr if hasattr(config.opt, \"process_features_lr\") else config.opt.lr},\n",
        "                 {'params': model.volume_net.parameters(), 'lr': config.opt.volume_net_lr if hasattr(config.opt, \"volume_net_lr\") else config.opt.lr}\n",
        "                ],\n",
        "                lr=config.opt.lr\n",
        "            )\n",
        "        else:\n",
        "            opt = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config.opt.lr)\n",
        "\n",
        "\n",
        "    # datasets\n",
        "    print(\"Loading data...\")\n",
        "    train_dataloader, val_dataloader, train_sampler = setup_dataloaders(config, distributed_train=is_distributed)\n",
        "\n",
        "    # experiment\n",
        "    experiment_dir, writer = None, None\n",
        "    if master:\n",
        "        experiment_dir, writer = setup_experiment(config, type(model).__name__, is_train=not args.eval)\n",
        "\n",
        "    # multi-gpu\n",
        "    if is_distributed:\n",
        "        model = DistributedDataParallel(model, device_ids=[device])\n",
        "\n",
        "    if not args.eval:\n",
        "        # train loop\n",
        "        n_iters_total_train, n_iters_total_val = 0, 0\n",
        "        for epoch in range(config.opt.n_epochs):\n",
        "            if train_sampler is not None:\n",
        "                train_sampler.set_epoch(epoch)\n",
        "\n",
        "            n_iters_total_train = one_epoch(model, criterion, opt, config, train_dataloader, device, epoch, n_iters_total=n_iters_total_train, is_train=True, master=master, experiment_dir=experiment_dir, writer=writer)\n",
        "            n_iters_total_val = one_epoch(model, criterion, opt, config, val_dataloader, device, epoch, n_iters_total=n_iters_total_val, is_train=False, master=master, experiment_dir=experiment_dir, writer=writer)\n",
        "\n",
        "            if master:\n",
        "                checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\", \"{:04}\".format(epoch))\n",
        "                os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "                torch.save(model.state_dict(), os.path.join(checkpoint_dir, \"weights.pth\"))\n",
        "\n",
        "            print(f\"{n_iters_total_train} iters done.\")\n",
        "    else:\n",
        "        if args.eval_dataset == 'train':\n",
        "            one_epoch(model, criterion, opt, config, train_dataloader, device, 0, n_iters_total=0, is_train=False, master=master, experiment_dir=experiment_dir, writer=writer)\n",
        "        else:\n",
        "            one_epoch(model, criterion, opt, config, val_dataloader, device, 0, n_iters_total=0, is_train=False, master=master, experiment_dir=experiment_dir, writer=writer)\n",
        "\n",
        "    print(\"Done.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = parse_args()\n",
        "    print(\"args: {}\".format(args))\n",
        "    main(args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd9nj5t-6gpC"
      },
      "source": [
        "##Clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgyai3WU6ijv"
      },
      "source": [
        "X = df_scaled1\n",
        "distortions = []\n",
        "inertias = []\n",
        "mapping1 = {}\n",
        "mapping2 = {}\n",
        "K = range(1, 15)\n",
        " \n",
        "for k in K:\n",
        "    # Building and fitting the model\n",
        "    scaler = MinMaxScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "    kmeanModel = KMeans(n_clusters=k).fit(X)\n",
        "    kmeanModel.fit(X)\n",
        " \n",
        "    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n",
        "                                        'euclidean'), axis=1)) / X.shape[0])\n",
        "    inertias.append(kmeanModel.inertia_)\n",
        " \n",
        "    mapping1[k] = sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n",
        "                                   'euclidean'), axis=1)) / X.shape[0]\n",
        "    mapping2[k] = kmeanModel.inertia_\n",
        "    \n",
        "\n",
        "plt.plot(K, distortions, 'bx-')\n",
        "plt.xlabel('Values of K')\n",
        "plt.title('The Elbow Method')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUxl9EdJ6vXv"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(df_scaled1)\n",
        "kmeans = KMeans(n_clusters=4, random_state=42).fit(X)\n",
        "\n",
        "labels = kmeans.labels_\n",
        "LABEL_COLOR_MAP = {0 : 'r',\n",
        "                   1 : 'k',\n",
        "                   2 : 'g', 3 : 'b'\n",
        "                   }\n",
        "\n",
        "label_color = [LABEL_COLOR_MAP[l] for l in labels]\n",
        "for col in df_scaled1.columns:\n",
        "    fig, ax = plt.subplots(figsize=(18, 6))\n",
        "    scatter = plt.scatter(df_scaled1.index, df_scaled1[col], c=label_color)\n",
        "    plt.legend(handles=scatter.legend_elements()[0], labels=set(labels))\n",
        "    plt.xticks(np.arange(0, 340, 100))\n",
        "    plt.title(6)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}